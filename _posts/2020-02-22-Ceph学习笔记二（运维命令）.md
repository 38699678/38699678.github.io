## Ceph学习笔记二(运维命令).md
#### 操纵集群
#### 监控集群
- 检查集群健康状态
  #ceph health
- 观察集群
  #ceph -w
- 检查集群的存储使用情况
``` bash
  [root@ceph-master ceph-cluster]# ceph df 
GLOBAL:
    SIZE        AVAIL       RAW USED     %RAW USED 
    7.8 TiB     6.1 TiB      1.7 TiB         22.29 
POOLS:
    NAME               ID     USED        %USED     MAX AVAIL     OBJECTS 
    elk-pool-data      1      241 GiB      9.27       2.3 TiB       66721 
    app-logs           2       39 GiB      1.64       2.3 TiB       10615 
    big-data-pool      3      104 GiB      4.20       2.3 TiB       29850 
    db-backup-pool     4      209 GiB      8.12       2.3 TiB       54550 
 ``` 
- 检查集群状态   
  **[root@ceph-master ceph-cluster]# ceph -s**  
  cluster:
    id:     960dcceb-6551-434b-9f05-52c3d405c73a  
    health: HEALTH_WARN  
            clock skew detected on mon.ceph-node2  
 
  services:  
    mon: 4 daemons, quorum ceph-master,ceph-node1,ceph-node2,ceph-node3  
    mgr: ceph-master(active), standbys: ceph-node1, ceph-node2  
    mds: cephfs-1/1/1 up  {0=ceph-node4=up:active}, 1 up:standby   
    osd: 5 osds: 4 up, 4 in  
 
  data:  
    pools:   4 pools, 224 pgs  
    objects: 158.6 k objects, 581 GiB  
    usage:   1.7 TiB used, 6.1 TiB / 7.8 TiB avail  
    pgs:     223 active+clean  
             1   active+clean+scrubbing+deep  
 
  io:  
    client:   67 MiB/s wr, 0 op/s rd, 37 op/s wr  
- 检查osd状态，你可以执行下列命令来确定 OSD 状态为 up 且 in ：  

**[root@ceph-master ceph-cluster]# ceph osd stat**  
5 osds: 4 up, 4 in; epoch: e213  
**[root@ceph-master ceph-cluster]# ceph osd dump**  
epoch 213  
fsid 960dcceb-6551-434b-9f05-52c3d405c73a  
created 2020-02-04 10:18:26.995418  
modified 2020-02-21 11:11:54.782054  
flags sortbitwise,recovery_deletes,purged_snapdirs  
crush_version 11  
full_ratio 0.95  
backfillfull_ratio 0.9  
nearfull_ratio 0.85  
require_min_compat_client jewel  
min_compat_client jewel  
require_osd_release mimic  
pool 1 'elk-pool-data' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 last_change 23 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd    
removed_snaps [1~3]  
max_osd 5  
osd.0 up   in  weight 1 up_from 192 up_thru 208 down_at 189 last_clean_interval [5,191) 172.18.178.90:6801/15406 172.18.178.90:6805/1015406 172.18.178.90:6806/1015406 172.18.178.90:6807/1015406 exists,up ebb2c28f-7ae8-42bf-8aa2-959c778bf868  
osd.1 up   in  weight 1 up_from 9 up_thru 208 down_at 0 last_clean_interval [0,0)   172.18.178.91:6800/12511 172.18.178.91:6801/12511 172.18.178.91:6802/12511   172.18.178.91:6803/12511 exists,up 4d20e845-6045-44b7-be31-831cb908f10c  
osd.2 up   in  weight 1 up_from 13 up_thru 208 down_at 0 last_clean_interval [0,0) 172.18.178.92:6800/12140 172.18.178.92:6801/12140 172.18.178.92:6802/12140 172.18.178.92:6803/12140 exists,up 3a76cc5c-621c-45d4-81da-fd978eb76ca1
osd.3 up   in  weight 1 up_from 17 up_thru 208 down_at 0 last_clean_interval [0,0) 172.18.178.93:6800/14295 172.18.178.93:6801/14295 172.18.178.93:6802/14295 172.18.178.93:6803/14295 exists,up 7a164374-4fd1-4026-b655-404263a04079
osd.4 down out weight 0 up_from 34 up_thru 104 down_at 112 last_clean_interval [0,0) 172.18.178.94:6800/23405 172.18.178.94:6801/23405 172.18.178.94:6802/23405 172.18.178.94:6803/23405 autoout,exists 53228f03-351d-4c29-97c1-e93409d09e2e  
**[root@ceph-master ceph-cluster]# ceph osd tree**  
ID  CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF   
 -1       9.76044 root default                                  
 -3       1.95209     host ceph-master                           
  0   hdd 1.95209         osd.0            up  1.00000 1.00000   
 -5       1.95209     host ceph-node1                            
  1   hdd 1.95209         osd.1            up  1.00000 1.00000   
 -7       1.95209     host ceph-node2                          
  2   hdd 1.95209         osd.2            up  1.00000 1.00000   
 -9       1.95209     host ceph-node3                          
  3   hdd 1.95209         osd.3            up  1.00000 1.00000   
-11       1.95209     host ceph-node4                          
  4   hdd 1.95209         osd.4          down        0 1.00000  

- 检查monitor状态  
#ceph mon stat  
#ceph mon dump  
要检查监视器的法定人数状态，执行下面的命令
#ceph quorum_status
``` bash
#ceph quorum_status
{"election_epoch":32,"quorum":[0,1,2,3],"quorum_names":["ceph-master","ceph-node1","ceph-node2","ceph-node3"],"quorum_leader_name":"ceph-master","monmap":{"epoch":2,"fsid":"960dcceb-6551-434b-9f05-52c3d405c73a","modified":"2020-02-04 10:18:31.903542","created":"2020-02-04 10:17:43.670009","features":{"persistent":["kraken","luminous","mimic","osdmap-prune"],"optional":[]},"mons":[{"rank":0,"name":"ceph-master","addr":"172.18.178.90:6789/0","public_addr":"172.18.178.90:6789/0"},{"rank":1,"name":"ceph-node1","addr":"172.18.178.91:6789/0","public_addr":"172.18.178.91:6789/0"},{"rank":2,"name":"ceph-node2","addr":"172.18.178.92:6789/0","public_addr":"172.18.178.92:6789/0"},{"rank":3,"name":"ceph-node3","addr":"172.18.178.93:6789/0","public_addr":"172.18.178.93:6789/0"}]}}
```
- 检查MDS状态   
元数据服务器为 Ceph 文件系统提供元数据服务，元数据服务器有两种状态：  
 up | down 和 active | inactive ，执行下面的命令查看元数据服务器状态为 up 且 active  

  #ceph mds stat
``` bash
[root@ceph-master ceph-cluster]# ceph mds stat
cephfs-1/1/1 up  {0=ceph-node4=up:active}, 1 up:standby
```

#### 监控 OSD 和归置组
- 监控osd  

#ceph osd stat
``` bash
[root@ceph-master ceph-cluster]# ceph osd stat
5 osds: 4 up, 4 in; epoch: e213
```
- 检查归置组集（pg）  
#ceph pg dump  
要根据指定归置组号查看哪些 OSD 位于 Acting Set 或 Up Set 里，执行：  
#ceph pg map {pg-num}  
``` bash
[root@ceph-master ceph-cluster]# ceph pg map 4.7f 
osdmap e213 pg 4.7f (4.7f) -> up [2,3,0] acting [2,3,0]
```
- 监控归置组状态  
#ceph pg stat
``` bash
[root@ceph-master ceph-cluster]# ceph pg stat
224 pgs: 224 active+clean; 740 GiB data, 2.2 TiB used, 5.6 TiB / 7.8 TiB avail; 5.2 MiB/s rd, 1.5 MiB/s wr, 133 op/s
```
- 定位对象  
#ceph osd map {poolname} {object-name}

#### 存储池
- 列出存储池  
#ceph osd lspools
- 创建存储池  
#ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated]
{replicated|erasure}：  
replicated：副本型存储池  
erasure：纠错型存储池  
- 设置存储池配额
#ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]  
例：  
#ceph osd pool set-quota data max_objects 10000
要取消配额，设置为0  
- 删除存储池  
#ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]  
- 查看存储池统计信息：  
#rados df
``` bash
[root@ceph-master ceph-cluster]# rados df 
POOL_NAME         USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD   WR_OPS      WR 
app-logs        39 GiB   10615      0  31845                  0       0        0     485 6.1 MiB   137244  40 GiB 
big-data-pool  104 GiB   29850      0  89550                  0       0        0    8681 144 MiB  1463554 130 GiB 
db-backup-pool 357 GiB   92723      0 278169                  0       0        0   17934 7.3 GiB   364747 361 GiB 
elk-pool-data  242 GiB   66831      0 200493                  0       0        0 3132846  40 GiB 35595264 1.7 TiB 

total_objects    200019
total_used       2.2 TiB
total_avail      5.6 TiB
total_space      7.8 TiB
```
- 拍下存储池快照  
#ceph osd pool mksnap {pool-name} {snap-name}  
- 删除存储池快照   
#ceph osd pool rmsnap {pool-name} {snap-name}  
- 调整存储池选项值  
#ceph osd pool set {pool-name} {key} {value}  
- 获取存储池选项值  
#ceph osd pool get {pool-name} {key}
``` bash
[root@ceph-master ceph-cluster]# ceph osd pool get elk-pool-data all
size: 3
min_size: 2
pg_num: 32
pgp_num: 32
crush_rule: replicated_rule
hashpspool: true
nodelete: false
nopgchange: false
nosizechange: false
write_fadvise_dontneed: false
noscrub: false
nodeep-scrub: false
use_gmt_hitset: 1
auid: 0
fast_read: 0
```
- 设置对象副本数  
#ceph osd pool set {poolname} size {num-replicas}  
- 获取对象副本数  
#ceph osd dump | grep 'replicated size'

#### 归置组（pg）
- 预定义pg_num    
用此命令创建存储池时：  
#ceph osd pool create {pool-name} pg_num  

确定 pg_num 取值是强制性的，因为不能自动计算。下面是几个常用的值：  
``` text
    少于 5 个 OSD 时可把 pg_num 设置为 128  
    OSD 数量在 5 到 10 个时，可把 pg_num 设置为 512  
    OSD 数量在 10 到 50 个时，可把 pg_num 设置为 4096  
    OSD 数量大于 50 时，你得理解权衡方法、以及如何自己计算 pg_num 取值  
    自己计算 pg_num 取值时可借助 pgcalc 工具  
```
随着 OSD 数量的增加，正确的 pg_num 取值变得更加重要，因为它显著地影响着集群的行为、以及出错时的数据持久性（即灾难性事件导致数据丢失的概率）  
- 设置归置组数量  
#ceph osd pool set {pool-name} pg_num {pg_num}  
- 获取归置组数量
#ceph osd pool get {pool-name} pg_num
- 获取归置组统计信息  
#ceph pg dump [--format {format}]  
可用格式有纯文本 plain （默认）和 json
- 获取卡住的归置组统计信息：  
要获取所有卡在某状态的归置组统计信息，执行命令：  
#ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format <format>] [-t|--threshold <seconds>]
``` text
Inactive （不活跃）归置组不能处理读写，因为它们在等待一个有最新数据的 OSD 复活且进入集群。

Unclean （不干净）归置组含有复制数未达到期望数量的对象，它们应该在恢复中。

Stale （不新鲜）归置组处于未知状态：存储它们的 OSD 有段时间没向监视器报告了（由 mon_osd_report_timeout 配置）。

可用格式有 plain （默认）和 json 。阀值定义的是，归置组被认为卡住前等待的最小时间（默认 300 秒）
```
- 获取一归置组运行图
#ceph pg map {pg-id}
- 洗刷归置组：  
#ceph pg scrub {pg-id}  
Ceph 检查原始的和任何复制节点，生成归置组里所有对象的目录，然后再对比，确保没有对象丢失或不匹配，并且它们的内容一致。   
- 恢复丢失的    
如果集群丢了一或多个对象，而且必须放弃搜索这些数据，你就要把未找到的对象标记为丢失（ lost ）。  如果所有可能的位置都查询过了，而仍找不到这些对象，你也许得放弃它们了。这可能是罕见的失败组合导致的，集群在写入完成前，未能得知写入是否已执行。当前只支持 revert 选项，它使得回滚到对象的前一个版本（如果它是新对象）或完全忽略它。要把 unfound 对象标记为 lost ，执行命令：  
#ceph pg {pg-id} mark_unfound_lost revert|delete

#### CRUSH map
- 获取CRUSH map：  
#ceph osd getcrushmap -o {compiled-crushmap-filename}  
Ceph 将把 CRUSH 输出（ -o ）到你指定的文件，由于 CRUSH 图是已编译的，所以编辑前必须先反编译
- 反编译 CRUSH map  
#crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename}  
Ceph 将反编译（ -d ）二进制 CRUSH 图，且输出（ -o ）到你指定的文件
- 编译CRUSH map  
#crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename}  
Ceph 将把已编译的 CRUSH 图保存到你指定的文件。
- 注入CRUSH map  
#ceph osd setcrushmap -i  {compiled-crushmap-filename}  
Ceph 将把你指定的已编译 CRUSH 图输入到集群。
