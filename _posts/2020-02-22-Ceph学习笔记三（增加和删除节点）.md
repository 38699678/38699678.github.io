## Ceph学习笔记三(增加osd节点) 
#### 增加osd节点 
新增节点为ceph-node6，提前在host文件中加入。  
- 准备工作。
  删除添加节点的磁盘数据。  
  #ceph-deploy purge ceph-node6
  在master节点，进入ceph的管理目录。  
  #cd /etc/ceph-cluster  
  编辑ceph.conf文件,在mon_initial_members中添加ceph-node6.   
``` bash
[global]
fsid = 0aef6103-520c-431b-b23a-2db203fb77b2
mon_initial_members = ceph-master ceph-node1 ceph-node2 ceph-node3 ceph-node4 ceph-node6
mon_host = ceph-master ceph-node1 ceph-node2 ceph-node3
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
```
- 在新节点中部署安装ceph程序  
  在新节点中安装ceph程序
  #ceph-deploy install ceph-node6 
- 拷贝新配置文件到集群中所有节点中  
  #ceph-deploy  --overwrite-conf config push ceph-master ceph-node{1,2,3,4,6}
- 初始化目标节点的磁盘
  查看目标节点磁盘
  #ceph-deploy disk list ceph-node6 
  初始化磁盘
  #ceph-deploy osd create --data /dev/sda ceph-node6
- 检查节点和集群状态。  
  **注意： 此方式适合于使用裸盘进行添加，如果要在LVM卷上创建 osd，则参数 --data 必须是 volume_group/lv_name，而不是卷的块设备的路径。(此操作要在安装目录中执行，例如：/usr/local/ceph-cluster)**  
  观察平衡过程  
   当新的节点加入集群，ceph 集群开始将部分现有的数据重新平衡到新加入的 osd 上，用下面的令可用观察平衡过程。
  ``` bash
  #ceph -w
  #watch ceph -s
  #watch ceph health
  ```
- 检查集群的存储容量    
  #rados df  
- 查看新加入的 osd  
  #ceph osd tree  
- **注意： 在生产环境中，一般不会再新节点加入 ceph 集群后，立即开始数据回填，这样会影响集群性能。所以我们需要设置一些标志位，来完成这个目的。**  
  #ceph osd set noin  
  #ceph osd set nobackfill  
  在用户访问的非高峰时，取消这些标志位，集群开始在平衡任务。  
  #ceph osd unset noin  
  #ceph osd unset nobackfill  


#### 删除osd节点  
- 进入要删除 osd 的主机，将 osd 的权重标记为 0  
  #ceph osd crush reweight osd.{osd-num} 0  
  **注: osd-num 通过 ceph osd tree 查看**
- 观察数据迁移  
  #ceph -w  
  #watch ceph -s  
**注: 待集群状态正常后再进行下一步操作**
- 将osd踢出集群  
  #ceph osd out {osd-num}  
**注: 观察集群状态，待集群状态正常后再进行下一步操作**
- 通过 systemctl 停止 osd 服务
``` bash
systemctl status ceph-osd@{osd-num}
systemctl stop ceph-osd@{osd-num}
systemctl disable ceph-osd@{osd-num}
systemctl status ceph-osd@{osd-num}
```
- 删除 CRUSH 图的对应 OSD 条目  
  #ceph osd crush remove osd.{osd-num}  
  **注: 删除 CRUSH 图的对应 OSD 条目，它就不再接收数据了。也可以反编译 CRUSH 图、删除 device 列表条目、删除对应的 host 桶条目或删除 host 桶（如果它在 CRUSH 图里，而且你想删除主机），重编译 CRUSH 图并应用它。**
- 删除 OSD 认证密钥  
  #ceph auth del osd.{osd-num}  
- 删除 OSD  
  #ceph osd rm {osd-num}  
- 处理配置文件  
  如果 ceph.conf 中记载了相关的内容，则需要修改配置文件并更新到其他主机，如果没有，则忽略此步骤。
- 卸载 osd 挂载的磁盘
``` bash
df -h
umount /var/lib/ceph/osd/ceph-{osd-num}
ceph-disk zap /dev/s{?}{?}
```
- 清除主机

  如果只想清除 /var/lib/ceph 下的数据、并保留 Ceph 安装包，可以用 purgedata 命令。  

  #ceph-deploy purgedata {hostname} [{hostname} ...]  

  要清理掉 /var/lib/ceph 下的所有数据、并卸载 Ceph 软件包，用 purge 命令。  

  #ceph-deploy purge {hostname} [{hostname} ...]  

- 修改 crushmap  
  剔除完 osd 之后，使用 ceph -s 状态正常，但是使用 ceph osd tree 查看列表的时，却发现已经没有osd的主机依然存在。  
  原因是 crushmap 的一些架构图，不会动态修改，所以需要手工的修改 crushmap。  
  #ceph osd  getcrushmap -o old.map  //导出crushmap  
  #crushtool -d old.map -o old.txt   //将crushmap转成文本文件，方便vim修改  
  #cp old.txt new.txt        //做好备份  
- 修改 new.txt, 将此 host 相关的内容删除
``` bash
  host host-name {
xxxxx
xxxxx
}
  host host-name {
xxxxx
xxxxx
}
root default {
item host-name weight 0.000
item host-name weight 0.000
}
```
  把以上的内容删除掉后，重新编译为二进制文件，并将二进制 map 应用到当前 map 中  
  #curshtool -c new.txt -o new.map  
  #ceph osd setcrushmap -i new.map  
此时ceph会重新收敛，等待收敛完毕后，ceph可正常使用
