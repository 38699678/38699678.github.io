# Ceph物理部署并在k8s环境中应用
### ceph简介
- 组成
ceph存储集群至少需要一个ceph monitor和两个osd守护进程。而运行ceph文件系统客户端时，必须要有元数据服务器（metadata server）。  
- OSDs: ceph osd守护进程的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他osd守护进程的心跳向ceph monitor提供一些监控信息。当ceph存储集群设定为2个副本时，至少需要2个osd守护进程。集群才能达到active+clean状态。默认是3个副本
- MDSs： ceph元数据服务器（MDS）为ceph文件系统存储元数据，也就是ceph块设备和ceph对象存储不使用MDS。
- ceph把客户端数据保存为存储池内的对象，通过使用CRUSH算法，ceph可以计算出哪个归置组（PG）应该持有指定的对象（object）。然后进一步计算出osd守护京城持有改归置组。CRUSH算法使得ceph存储集群能够动态的伸缩、再均衡和修复。
- 注：  
OSD 数量较多（如 20 个以上）的主机会派生出大量线程，尤其是在恢复和重均衡期间。很多 Linux 内核默认的最大线程数较小（如 32k 个），如果您遇到了这类问题，可以把 kernel.pid_max 值调高些。理论最大值是 4194303 。例如把下列这行加入 /etc/sysctl.conf 文件：
``` bash
kernel.pid_max = 4194303
```

### 部署前准备
当前我们推荐：  
    4.1.4 or later  
    3.16.3 or later (rbd deadlock regression in 3.16.[0-2])  
    NOT v3.15.* (rbd deadlock regression)  
    3.14.*  
- 升级内核
``` bash
#rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
#rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
#yum --disablerepo="*" --enablerepo="elrepo-kernel" list available
#yum --enablerepo=elrepo-kernel install kernel-ml
```
- 设置GRUB默认的内核版本：为了让新安装的内核成为默认启动选项，你需要如下修改 GRUB 配置,打开并编辑 /etc/default/grub 并设置 GRUB_DEFAULT=0.意思是 GRUB 初始化页面的第一个内核将作为默认内核.
``` bash
# more /etc/default/grub 
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=0
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT="console"
GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet"
GRUB_DISABLE_RECOVERY="true"
```
- 重新构建内核配置
``` bash
#grub2-mkconfig -o /boot/grub2/grub.cfg
```
- 重启机器，检查当前centos的内核版本
``` bash
# uname -r 
5.5.0-1.el7.elrepo.x86_64
```

### 部署ceph
- 架构  
  ceph master： 172.18.178.90  
  ceph node1: 172.18.178.91  
  ceph node2: 172.18.178.92  
  ceph node3: 172.18.178.93  
  ceph node4: 172.18.178.94  
  ceph node5: 172.18.178.95  
  ceph node6: 172.18.178.96  
- master节点安装ceph-deploy  
``` bash
#sudo yum install -y yum-utils && sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ && sudo yum install --nogpgcheck -y epel-release && sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 && sudo rm /etc/yum.repos.d/dl.fedoraproject.org*
#cat >/etc/yum.repos.d/ceph.repo <EOF
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-nautilus/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
>EOF
#yum install -y ceph-deploy
```
- 所有节点安装ntp  
  所有Ceph节点上安装 NTP 服务（特别是 Ceph Monitor 节点），以免因时钟漂移导致故障
``` bash
#yum install ntp ntpdate ntp-doc
```
- 编辑hosts文件，并保存到ceph的所有节点
``` bash 
[root@ceph-master ceph-cluster]# cat /etc/hosts
172.18.178.90 ceph-master
172.18.178.91 ceph-node1
172.18.178.92 ceph-node2
172.18.178.93 ceph-node3
172.18.178.94 ceph-node4
172.18.178.95 ceph-node5
172.18.178.96 ceph-node6
```
- 生成密钥并拷贝的所有节点，实现无密码登陆  
#ssh-keygen  
#ssh-copy-id   

### 部署ceph
- 在ceph管理节点创建一个目录，用于保存ceph-deploy生成的配置文件和密钥对,后面ceph-deploy命令请确保在此目录下执行。
``` bash
#cd /etc/ceph-cluster && cd /etc/ceph-cluster
```
- 初始化ceph集群节点。
``` bash
#ceph-deploy new ceph-master ceph-node1 ceph-node2 ceph-node3 
``` 
- 在各节点安装ceph程序
``` bash
#ceph-deploy install ceph-master ceph-node1 ceph-node2 ceph-node3
```
报错
[ceph_deploy][ERROR ] RuntimeError: NoSectionError: No section: 'ceph'  
``` bash
yum remove ceph-release  
rm /etc/yum.repos.d/ceph.repo.rpmsave  
```


- 配置初始 monitor(s)、并收集所有密钥
``` bash 
#ceph-deploy mon create-initial 
```
如果报以下错误  
``` bash  
admin_socket: exception getting command descriptions: [Errno 2] No such file or directory
#ceph-deploy --overwrite-conf config push  ceph-master ceph-node1 ceph-node2 ceph-node3
[ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: ceph-node3
[ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

解决办法
vi ceph.conf
加入 public network = 10.5.19.0/24
# /usr/bin/ceph-deploy --overwrite-conf config push ceph-master ceph-1 ceph-2
在每个节点执行，修改hostname
hostnamectl set-hostname ceph-master
#pkill ceph
重新执行该命令
```
- 执行成功后，可以在当前目录查看密钥文件
``` bash 
[root@pub-server-ceph-00 ceph-cluster]# ls |grep keyring
ceph.bootstrap-mds.keyring
ceph.bootstrap-mgr.keyring
ceph.bootstrap-osd.keyring
ceph.bootstrap-rgw.keyring
ceph.client.admin.keyring
ceph.mon.keyring
```

- 部署一个管理进程
``` bash
#ceph-deploy mgr create ceph-master
```
- 将配置文件和密钥复制到集群各节点
``` bash
#ceph-deploy admin ceph-master ceph-node1 ceph-node2 ceph-node3 
```
- 查看节点磁盘存储情况
``` bash
[ceph@ceph-admin ceph]$ ceph-deploy disk list ceph-master
[ceph@ceph-admin ceph]$ ceph-deploy disk list ceph-node1
[ceph@ceph-admin ceph]$ ceph-deploy disk list ceph-node2
[ceph@ceph-admin ceph]$ ceph-deploy disk list ceph-node3
```
- 擦除指定storage节点，指定硬盘的数据
``` bash
ceph-deploy disk zap node-3 /dev/sdb
ceph-deploy disk zap node-2 /dev/sdb
ceph-deploy disk zap node-1 /dev/sdb
```
- 创建osd
``` bash
ceph-deploy osd create ceph-master  --data /dev/sdb
ceph-deploy osd create ceph-node1  --data /dev/sdb
ceph-deploy osd create ceph-node2  --data /dev/sdb
ceph-deploy osd create ceph-node3  --data /dev/sdb
``` 
- 检查osd状态
``` bash
[root@ceph-master ceph-cluster]# ceph osd status
+----+-------------+-------+-------+--------+---------+--------+---------+-----------+
| id |     host    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+-------------+-------+-------+--------+---------+--------+---------+-----------+
| 0  | ceph-master | 1026M | 1997G |    0   |     0   |    0   |     0   | exists,up |
| 1  |  ceph-node1 | 1026M | 1997G |    0   |     0   |    0   |     0   | exists,up |
| 2  |  ceph-node2 | 1026M | 1997G |    0   |     0   |    0   |     0   | exists,up |
| 3  |  ceph-node3 | 1026M | 1997G |    0   |     0   |    0   |     0   | exists,up |
+----+-------------+-------+-------+--------+---------+--------+---------+-----------+
```
以上就部署完成ceph集群

### poll基本操作
- pool是ceph存储数据时的逻辑分区，它起到namespace的作用。 每个pool包含一定数量的PG，PG里的对象被映射到不同的OSD上，因此pool是分布到整个集群的。
- 创建存储池  
创建pool之前可更改两个pg默认配置：pg_num是指定创建的pg的个数，会有一组编号，然后pgp_num就是控制pg到osd的映射分布。一般最好将pgp_num设置成一样。安装完ceph集群后默认会创建一个名字叫rbd的存储池，可自己创建存储池  
- 有两种类型的pool，一种是副本（replicated pools），一种是纠删码（erasure code pools）。 副本池是我们最常用的类型。  

- 查询pool列表
``` bash
#ceph osd lspools
```
- 创建存储池
``` bash
ceph osd pool create elk-pool-data 32 32 replicated
```
- 删除存储池
``` bash
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
```
- 重命名存储池
``` bash
ceph osd pool rename {current-pool-name} {new-pool-name}
```
- 快照
pool也支持snapshot功能。可以运行ceph osd pool mksnap命令创建pool的快照，并且在必要的时候恢复它。 还可以设置pool的拥有者属性，从而进行访问控制。
-  创建快照
``` bash
ceph osd pool mksnap {pool-name} {snap-name}
```
- 删除快照
``` bash
ceph osd pool rmsnap {pool-name} {snap-name}
```
- rbd命令操作快照
``` bash
rbd -p <pool name> snap create {snap-name}     Create a snapshot.
rbd -p <pool name> snap list                   Dump list of image snapshots.
rbd -p <pool name> snap protect {snap-name}    Prevent a snapshot from being deleted.
rbd -p <pool name> snap purge                  Deletes all snapshots.
rbd -p <pool name> snap remove {snap-name}     Deletes a snapshot.
rbd -p <pool name> snap rename  {snap} {snap1} Rename a snapshot.
rbd -p <pool name> snap rollback {snap-name}   Rollback image to snapshot.
rbd -p <pool name> snap unprotect {snap-name}  Allow a snapshot to be deleted.
```
- 获取/设置存储池属性
``` bash
ceph osd pool get {pool-name} {key}
ceph osd pool set {pool-name} {key} {value}
```
列出一些比较常见的属性：
    size - 副本数
    pg_num - pg数量
    pgp_num - pg放置数量
    crush_ruleset - 规则id

- 设置允许容量限制为10GB:
``` bash
ceph osd pool set-quota test-pool max_bytes $((10 * 1024 * 1024 * 1024))
```
取消配额限制只需要把对应值设为0即可。


### 删除ceph集群
如果在某些地方碰到麻烦，想从头再来，可以用下列命令清除配置：
``` bash
ceph-deploy purge {ceph-node} [{ceph-node}] #清除所有配置，并删除ceph程序
ceph-deploy purgedata {ceph-node} [{ceph-node}] #值删除配置
ceph-deploy forgetkeys
rm ceph.*
```
- 删除绑定的ceph卷
``` bash
#fdisk -l
Disk /dev/mapper/ceph--51bd309b--fddf--4b28--b0b3--3839a65306b1-osd--block--32ddf6e8--0c0e--4f38--8b78--8e58048fdf6c: 2146.4 GB, 2146409906176 bytes, 4192206848 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

[root@ceph-node2 ~]# vgremove /dev/mapper/ceph--51bd309b--fddf--4b28--b0b3--3839a65306b1
Do you really want to remove volume group "ceph-51bd309b-fddf-4b28-b0b3-3839a65306b1" containing 1 logical volumes? [y/n]: y
Do you really want to remove active logical volume ceph-51bd309b-fddf-4b28-b0b3-3839a65306b1/osd-block-32ddf6e8-0c0e-4f38-8b78-8e58048fdf6c? [y/n]: y
  Logical volume "osd-block-32ddf6e8-0c0e-4f38-8b78-8e58048fdf6c" successfully removed
  Volume group "ceph-51bd309b-fddf-4b28-b0b3-3839a65306b1" successfully removed

ceph-deploy purge ceph-node1 ceph-node2 ceph-node3 ceph-master
rm -rf /var/lib/ceph/* 
```

### ceph应用
- 在master中，获取客户端连接的token
``` bash
[root@ceph-master ceph-cluster]# more /etc/ceph-cluster/ceph.client.admin.keyring 
[client.admin]
	key = AQBy1DheoqVeOxAA2KXMLTFixRqqOPRpkSa71g==
	caps mds = "allow *"
	caps mgr = "allow *"
	caps mon = "allow *"
	caps osd = "allow *"
```
- 创建存储池,并初始化存储池
``` bash
#ceph osd pool create elk-pool-data
#rbd pool init elk-pool-data
```
- ceph-csi需要一个存储在Kubernetes中的ConfigMap来定义Ceph集群的Ceph monitor地址。收集Ceph集群唯一的fsid和监视器地址： 
``` bash
[root@ceph-master ceph-cluster]# ceph mon dump
dumped monmap epoch 2
epoch 2
fsid 960dcceb-6551-434b-9f05-52c3d405c73a
last_changed 2020-02-04 10:18:31.903542
created 2020-02-04 10:17:43.670009
0: 172.18.178.90:6789/0 mon.ceph-master
1: 172.18.178.91:6789/0 mon.ceph-node1
2: 172.18.178.92:6789/0 mon.ceph-node2
3: 172.18.178.93:6789/0 mon.ceph-node3
```
- 在k8s集群中创建configmap，定义ceph集群信息
``` bash
 cat <<EOF > csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "960dcceb-6551-434b-9f05-52c3d405c73a", #此处为ceph的fsid 
        "monitors": [
          "172.18.178.90:6789",
          "172.18.178.91:6789",
          "172.18.178.92:6789",
          "172.18.178.93:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config

EOF
```
- 创建secrets，用于k8s访问ceph集群
``` bash
[root@ylcs-k8s-master src]# more ceph-secret.yml 
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
  namespace: kube-ops 
stringData:
  userID: admin
  userKey: AQBy1DheoqVeOxAA2KXMLTFixRqqOPRpkSa71g==  此处为之前获得的token值
```
- 下载rbac配置文件
``` bash
#wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
#wget  https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml
- 下载ceph-csi驱动文件
``` bash
#wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
#wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
- 部署以上资源文件
``` bash
#kubectl apply -f .
```
- 查看驱动pods是否部署完成
``` bash
[root@ylcs-k8s-master elasticsearch]# kubectl get pods 
NAME                                        READY   STATUS    RESTARTS   AGE
centos7                                     1/1     Running   71         2d23h
csi-rbdplugin-bl8ng                         3/3     Running   0          3h15m
csi-rbdplugin-cxljk                         3/3     Running   0          3h15m
csi-rbdplugin-provisioner-66dfffd45-2brv5   6/6     Running   0          3h15m
csi-rbdplugin-provisioner-66dfffd45-5ptdf   6/6     Running   0          3h15m
csi-rbdplugin-provisioner-66dfffd45-bzgjz   6/6     Running   0          3h15m
csi-rbdplugin-tf6r6                         3/3     Running   0          3h15m
csi-rbdplugin-vd595                         3/3     Running   0          3h15m
csi-rbdplugin-vqf5s                         3/3     Running   0          3h15m
csi-rbdplugin-z8dc5                         3/3     Running   0          3h15m
```
- 在k8s master中创建storageclass 部署文件
``` bash
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: elk-data-db  #storageclass的名称
   namespace: kube-ops 
provisioner: rbd.csi.ceph.com  
parameters:
  clusterID: "960dcceb-6551-434b-9f05-52c3d405c73a" #此处为ceph fsid
  pool: elk-pool-data  #此处为存储池
  csi.storage.k8s.io/provisioner-secret-name: ceph-secret
  csi.storage.k8s.io/provisioner-secret-namespace: kube-ops
  csi.storage.k8s.io/node-stage-secret-name: ceph-secret
  csi.storage.k8s.io/node-stage-secret-namespace: kube-ops
reclaimPolicy: Delete
mountOptions:
   - discard
```
- 修改之前的elasticsearch的部署文件
``` bash
---
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          apps: elasticsearch
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: elk-data-db
        resources:
          requests:
            storage: 2000Gi
```
- 发布elasticsearch。
``` bash
[root@ylcs-k8s-master elasticsearch]# kubectl get pvc -n kube-ops 
NAME                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-elasticsearch-0   Bound    pvc-7e0a114a-0f5d-404f-a33b-202c86bb10d7   2000Gi     RWO            elk-data-db    3h1m
data-elasticsearch-1   Bound    pvc-352ccc00-f7ff-4d52-9fa8-bd9a8bb14523   2000Gi     RWO            elk-data-db    157m
data-elasticsearch-2   Bound    pvc-13692b9f-c05e-4ebc-9a82-1560fe96072e   2000Gi     RWO            elk-data-db    157m
[root@ylcs-k8s-master elasticsearch]# kubectl get pods  -n kube-ops 
NAME                    READY   STATUS    RESTARTS   AGE
elasticsearch-0         1/1     Running   0          158m
elasticsearch-1         1/1     Running   0          158m
elasticsearch-2         1/1     Running   0          157m
kibana-6c88958f-zxcwr   1/1     Running   0          6h49m
```
### 普通pod应用ceph
- 创建storageclass文件
``` bash
$ cat <<EOF > csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
$ kubectl apply -f csi-rbd-sc.yaml
```
- 创建pvc文件
``` bash
$ cat <<EOF > pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
$ kubectl apply -f pvc.yaml
```
- 创建pod测试文件
``` bash 
$ cat <<EOF > pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
$ kubectl apply -f pod.yaml
``` 
- 以上操作完成后，可以在pod所在节点中看到一个ceph的硬件
``` bash
#df -Th
/dev/rbd0               ext4      2.0T  712M  2.0T   1% /opt/logs
如果想要查看该硬件，可以通过挂载方式查看
```
